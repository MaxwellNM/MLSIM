{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "import numpy as np \n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from model.MDBN import MDBN\n",
    "from parameters import HYPERPARAMS, OPTIMIZER, DATASET,TRAINING\n",
    "from preprocess import *\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix,precision_recall_curve,auc\n",
    "from sklearn.preprocessing import normalize as nz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 int\n",
      "1   float\n",
      "2     hyperopt_param\n",
      "3       Literal{nb_hiden_node1}\n",
      "4       quniform\n",
      "5         Literal{10}\n",
      "6         Literal{18}\n",
      "7        q =\n",
      "8         Literal{5}\n"
     ]
    }
   ],
   "source": [
    "# define the search space\n",
    "fspace = {\n",
    "    #'optimizer': hp.choice('optimizer', OPTIMIZER.optimizer),\n",
    "    #'optimizer_param': hp.uniform('optimizer_param', OPTIMIZER.optimizer_param['min'], OPTIMIZER.optimizer_param['max']),\n",
    "    'learning_rate': hp.uniform('learning_rate', OPTIMIZER.learning_rate['min'], OPTIMIZER.learning_rate['max']),    \n",
    "    'nb_hiden_node1': scope.int(hp.quniform('nb_hiden_node1', OPTIMIZER.nb_hiden_node1['min'], OPTIMIZER.nb_hiden_node1['max'], q=5)),\n",
    "    'nb_hiden_node2': scope.int(hp.quniform('nb_hiden_node2', OPTIMIZER.nb_hiden_node2['min'], OPTIMIZER.nb_hiden_node2['max'],  q=5)),    \n",
    "    'epochs': scope.int(hp.quniform('epochs', OPTIMIZER.epochs['min'], OPTIMIZER.epochs['max'],  q=5)), \n",
    "    'batch_size': scope.int(hp.quniform('batch_size', OPTIMIZER.batch_size['min'], OPTIMIZER.batch_size['max'], q=5)),  \n",
    "    'C': hp.uniform('C', OPTIMIZER.C['min'], OPTIMIZER.C['max'])\n",
    "}\n",
    "\n",
    "print(fspace[\"nb_hiden_node1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarized data\n",
    "\"\"\" transform features in binary format to train in the DBN.\n",
    "@Input: Hint: matrix of preferences\n",
    "@Output: xp : binarized data to be trained by te model\n",
    "\"\"\"\n",
    "def binarized_features(data):\n",
    "    enc = {}\n",
    "    (m,n) = data.shape\n",
    "    nb_features = 3*n\n",
    "    enc[0] = [0, 0, 1]\n",
    "    enc[1] = [0, 1, 0]\n",
    "    enc[2] = [0, 1, 1]\n",
    "    enc[3] = [1, 0, 0]\n",
    "    xp = np.zeros((data.shape[0],nb_features))\n",
    "    for i,row in enumerate(data):\n",
    "        l = [enc[elt] for elt in row]\n",
    "        xp[i] = np.hstack(l)\n",
    "    return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape data  (507, 6)\n",
      "shape target  (507, 1)\n",
      "405 6\n",
      "[[0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 1. 1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# parse arguments\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"-m\", \"--max_evals\", required=True, help=\"Maximum number of evaluations during hyperparameters search\")\n",
    "#args = parser.parse_args()\n",
    "#max_evals = int(args.max_evals)\n",
    "current_eval = 1\n",
    "train_history = []\n",
    "SEED = 12345\n",
    "\n",
    "dbn_feats_sg = np.load(\"./output/data/phili2017/Hint_sg.npy\")\n",
    "data = binarized_features(dbn_feats_sg) #np.load(DATASET.path_data2[DATASET.LS][\"data\"])\n",
    "target = np.load(\"./output/data/phili2017/labels/label_sg.npy\")#np.load(DATASET.path_data2[DATASET.LS][\"label\"])\n",
    "if (np.shape(target)!=2):\n",
    "    target = np.expand_dims(target,1)\n",
    "print(\"shape data \",np.shape(data))\n",
    "print(\"shape target \",np.shape(target))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,test_size=0.2,random_state=SEED)\n",
    "n = np.shape(X_train)[0]\n",
    "m = np.shape(X_train)[1]\n",
    "print(n,m)\n",
    "print (data[:9,:])\n",
    "print(target[:9,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defint the fucntion to minimize (will train the model using the specified hyperparameters)\n",
    "def function_to_minimize(hyperparams,\n",
    "        nb_hiden_node1=HYPERPARAMS.nb_hiden_node1, nb_hiden_node2=HYPERPARAMS.nb_hiden_node2, \n",
    "        learning_rate=HYPERPARAMS.learning_rate,\n",
    "        batch_size=HYPERPARAMS.batch_size,\n",
    "        epochs=HYPERPARAMS.epochs,\n",
    "        C=HYPERPARAMS.C):\n",
    "    if 'learning_rate' in hyperparams: \n",
    "        learning_rate = hyperparams['learning_rate']\n",
    "    if 'batch_size' in hyperparams: \n",
    "        batch_size = hyperparams['batch_size']\n",
    "    if 'nb_hiden_node1' in hyperparams: \n",
    "        nhid_param1 = hyperparams['nb_hiden_node1']\n",
    "    if 'nb_hiden_node2' in hyperparams: \n",
    "        nhid_param2 = hyperparams['nb_hiden_node2']   \n",
    "    if 'C' in hyperparams: \n",
    "        C = hyperparams['C'] \n",
    "    if 'epochs' in hyperparams: \n",
    "        epochs = hyperparams['epochs']        \n",
    "    if 'optimizer' in hyperparams:\n",
    "        optimizer = hyperparams['optimizer']\n",
    "    if 'optimizer_param' in hyperparams:\n",
    "        optimizer_param = hyperparams['optimizer_param']\n",
    "    global current_eval \n",
    "    global max_evals\n",
    "    print( \"#################################\")\n",
    "    print( \"       Evaluation {} of {}\".format(current_eval, max_evals))\n",
    "    print( \"#################################\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        dbn = MDBN(input_data=X_train, label=y_train, input_size=m, hidden_layer_sizes=[nhid_param1, nhid_param2],batch_size=batch_size, learning_rate=learning_rate,epochs=epochs, C=C)\n",
    "        start_time = time.time()\n",
    "        print (\"Training the Hybrid Deep Belief Net model \\n................\")\n",
    "        #Unsupervized Train the RBM\n",
    "        dbn.train(X_train)\n",
    "        #Fine tuning training SVM\n",
    "        dbn.finetune(X_train,y_train.ravel(),\"finetuning Train data\",rng=SEED,bytraining=True)\n",
    "        aucc = dbn.evaluate(X_test,y_test.ravel(),\"finetuning data\",verbose=False)\n",
    "        training_time = int(round(time.time() - start_time))\n",
    "        current_eval += 1\n",
    "        train_history.append({'accuracy':aucc, 'learning_rate':learning_rate, 'nb_hiden_node1':nhid_param1, 'c':nhid_param2, 'batch_size':batch_size,'epochs':epochs,'C':C,\n",
    "                                   'time':training_time})\n",
    "    except Exception as e:\n",
    "        # exception occured during training, saving history and stopping the operation\n",
    "        print( \"#################################\")\n",
    "        print( \"Exception during training: {}\".format(str(e)))\n",
    "        print( \"Saving train history in train_history.npy\")\n",
    "        np.save(\"train_history.npy\", train_history)\n",
    "        exit()\n",
    "    return {'loss': -aucc, 'time': training_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################                    \n",
      "       Evaluation 1 of 5                             \n",
      "#################################                    \n",
      "Training the Hybrid Deep Belief Net model            \n",
      "................\n",
      "Training with SVM                                    \n",
      "Finetuning with SVM                                  \n",
      "[LibSVM]                                             \n",
      "Time fitting the data:                               \n",
      "0.023313045501708984                                 \n",
      "#################################                                                \n",
      "       Evaluation 2 of 5                                                         \n",
      "#################################                                                \n",
      "Training the Hybrid Deep Belief Net model                                        \n",
      "................\n",
      "Training with SVM                                                                \n",
      "Finetuning with SVM                                                              \n",
      "[LibSVM]                                                                         \n",
      "Time fitting the data:                                                           \n",
      "0.027119874954223633                                                             \n",
      "#################################                                                \n",
      "       Evaluation 3 of 5                                                         \n",
      "#################################                                                \n",
      "Training the Hybrid Deep Belief Net model                                        \n",
      "................\n",
      "Training with SVM                                                                \n",
      "Finetuning with SVM                                                              \n",
      "[LibSVM]                                                                         \n",
      "Time fitting the data:                                                           \n",
      "0.02231597900390625                                                              \n",
      "#################################                                                \n",
      "       Evaluation 4 of 5                                                         \n",
      "#################################                                                \n",
      "Training the Hybrid Deep Belief Net model                                        \n",
      "................\n",
      "Training with SVM                                                                \n",
      "Finetuning with SVM                                                              \n",
      "[LibSVM]                                                                         \n",
      "Time fitting the data:                                                           \n",
      "0.01728081703186035                                                              \n",
      "#################################                                                \n",
      "       Evaluation 5 of 5                                                         \n",
      "#################################                                                \n",
      "Training the Hybrid Deep Belief Net model                                        \n",
      "................\n",
      "Training with SVM                                                                \n",
      "Finetuning with SVM                                                              \n",
      "[LibSVM]                                                                         \n",
      "Time fitting the data:                                                           \n",
      "0.026105880737304688                                                             \n",
      "100%|██████████| 5/5 [13:19<00:00, 159.95s/trial, best loss: -0.8590686274509804]\n"
     ]
    }
   ],
   "source": [
    "# lancer la recherche des  hyperparametres\n",
    "max_evals =5\n",
    "trials = Trials()\n",
    "best_trial = fmin(fn=function_to_minimize, space=fspace, algo=tpe.suggest, max_evals=max_evals, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "      Best parameters found\n",
      "#################################\n",
      "{'C': 0.6050592537264041,\n",
      " 'aucc': 85.90686274509804,\n",
      " 'batch_size': 85.0,\n",
      " 'epochs': 715.0,\n",
      " 'learning_rate': 0.6344169903493311,\n",
      " 'nb_hiden_node1': 15.0,\n",
      " 'nb_hiden_node2': 10.0,\n",
      " 'time': 143}\n",
      "#################################\n"
     ]
    }
   ],
   "source": [
    "# get some additional information and print the best parameters\n",
    "for trial in trials.trials:\n",
    "    if trial['misc']['vals']['learning_rate'][0] == best_trial['learning_rate'] and \\\n",
    "            trial['misc']['vals']['nb_hiden_node1'][0] == best_trial['nb_hiden_node1'] and \\\n",
    "            trial['misc']['vals']['nb_hiden_node2'][0] == best_trial['nb_hiden_node2'] and \\\n",
    "            trial['misc']['vals']['batch_size'][0] == best_trial['batch_size'] and \\\n",
    "            trial['misc']['vals']['epochs'][0] == best_trial['epochs'] and \\\n",
    "            trial['misc']['vals']['C'][0] == best_trial['C'] :\n",
    "        best_trial['aucc'] = -trial['result']['loss'] * 100\n",
    "        best_trial['time'] = trial['result']['time']\n",
    "print( \"#################################\")\n",
    "print( \"      Best parameters found\")\n",
    "print( \"#################################\")\n",
    "pprint.pprint(best_trial)\n",
    "print( \"#################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
